diff -r -U5 orig/xen-4.8.1/xen/arch/x86/hvm/hvm.c xen-4.8.1/xen/arch/x86/hvm/hvm.c
--- orig/xen-4.8.1/xen/arch/x86/hvm/hvm.c	2017-04-10 15:21:48.000000000 +0200
+++ xen-4.8.1/xen/arch/x86/hvm/hvm.c	2017-05-26 21:47:56.866453519 +0200
@@ -4300,18 +4300,97 @@
 #undef do_arch_1
 
 #undef HYPERCALL
 #undef COMPAT_CALL
 
+/* also does a compiler memory barrier for good measure */
+#define cpuid_pipeline_flush() asm volatile ("mov $0, %%eax\n\tcpuid" : /*out*/ : /*in*/ : "ax", "bx", "cx", "dx", "memory", "cc");
+
+char secret_data[16] = { 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1 };
+
+char loadme_area[16000];
+#define loadme (loadme_area + 8128)
+
+unsigned long load_loadme_unused0(unsigned long index) { return loadme[(secret_data[index]&1)<<10]; }
+unsigned long load_loadme_unused1(unsigned long index) { return loadme[(secret_data[index]&1)<<10]; }
+unsigned long load_loadme_unused2(unsigned long index) { return loadme[(secret_data[index]&1)<<10]; }
+unsigned long load_loadme_unused3(unsigned long index) { return loadme[(secret_data[index]&1)<<10]; }
+unsigned long load_loadme_unused4(unsigned long index) { return loadme[(secret_data[index]&1)<<10]; }
+unsigned long load_loadme_unused5(unsigned long index) { return loadme[(secret_data[index]&1)<<10]; }
+unsigned long load_loadme_unused6(unsigned long index) { return loadme[(secret_data[index]&1)<<10]; }
+unsigned long load_loadme_unused7(unsigned long index) { return loadme[(secret_data[index]&1)<<10]; }
+unsigned long load_loadme_unused8(unsigned long index) { return loadme[(secret_data[index]&1)<<10]; }
+unsigned long load_loadme_unused9(unsigned long index) { return loadme[(secret_data[index]&1)<<10]; }
+
+unsigned long timed_load_loadme(unsigned long bit) {
+    long res, dummy;
+    asm volatile (
+        "mov $0, %%rax\n\t"
+        "cpuid\n\t"
+
+        "rdtsc\n\t"
+        "mov %%rax, %0\n\t"
+
+        "mov $0, %%rax\n\t"
+        "cpuid\n\t"
+
+        "mov (%2), %1\n\t"
+
+        "mov $0, %%rax\n\t"
+        "cpuid\n\t"
+
+        "rdtsc\n\t"
+        "sub %%rax, %0\n\t"
+
+        "mov $0, %%rax\n\t"
+        "cpuid\n\t"
+    : "=&r"(res), "=&r"(dummy) /* outputs */
+    : "r"(loadme + (bit ? (1<<10) : 0)) /* inputs */
+    : "memory", "cc", "ax", "bx", "cx", "dx" /* clobber */
+    );
+    return -res;
+}
+
+unsigned long (*nop_pointer_area[1000])(unsigned long);
+#define nop_pointer (nop_pointer_area + 518)
+
+unsigned long nop_fn(unsigned long index) { return 0; }
+
+unsigned long call_nop_indirectly(unsigned long arg) {
+    unsigned long res;
+    cpuid_pipeline_flush();
+    *nop_pointer = nop_fn;
+    cpuid_pipeline_flush();
+    clflush(nop_pointer);
+    cpuid_pipeline_flush();
+    res = (*nop_pointer)(arg);
+    cpuid_pipeline_flush();
+    return res;
+}
+
 int hvm_do_hypercall(struct cpu_user_regs *regs)
 {
     struct vcpu *curr = current;
     struct domain *currd = curr->domain;
     struct segment_register sreg;
     int mode = hvm_guest_x86_mode(curr);
     unsigned long eax = regs->_eax;
 
+    if (eax == 0x13370000) {
+        regs->_eax = timed_load_loadme(regs->rdi);
+        clflush(loadme);
+        clflush(loadme + (1<<10));
+        return HVM_HCALL_completed;
+    } else if (eax == 0x13370001) {
+        regs->_eax = call_nop_indirectly(regs->rdi);
+        return HVM_HCALL_completed;
+    } else if (eax == 0x13370002) { /* cacheline load */
+        cpuid_pipeline_flush(); /* speculation guard */
+        regs->_eax = loadme[0];
+        return HVM_HCALL_completed;
+    }
+
     switch ( mode )
     {
     case 8:
         eax = regs->rax;
         /* Fallthrough to permission check. */
